{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyxdf \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from lmfit.models import Model\n",
    "from os import listdir, getcwd\n",
    "from os.path import isfile, join\n",
    "from scipy import stats\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted pupil dilation, $d(Y)$, caused by luminance $Y$, is computed with the following equation: $𝑑(𝑌) = 𝑎 · 𝑒^{−𝑏·𝑌} + c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pupil_func(x, a, b, c):\n",
    "    return a * np.exp(-b * x) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(file):\n",
    "    streams, header = pyxdf.load_xdf(file)\n",
    "    dfs = {}\n",
    "    for stream in streams:\n",
    "        stream_name = stream['info']['name'][0]\n",
    "        stream_channels = {channel['label'][0]: i for i, channel in enumerate(stream['info']['desc'][0]['channels'][0]['channel'])}\n",
    "        stream_data = stream['time_series']\n",
    "        data_dict = {key: np.array(stream_data)[:, index] for key, index in stream_channels.items()}\n",
    "        data_dict['time'] = np.round(np.array(stream['time_stamps']), decimals=4)\n",
    "        dfs[stream_name] = pd.DataFrame(data_dict).drop_duplicates(subset=['time']).reset_index(drop=True)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "accom_time = pd.to_timedelta(0.5, unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gaze_luminance_data(stream_df):\n",
    "    pupil = stream_df['GazeStream'].loc[(stream_df['GazeStream']['LeftEyeIsBlinking'] == 0) & (stream_df['GazeStream']['RightEyeIsBlinking'] == 0) & (stream_df['GazeStream']['LeftPupilDiameter'] > 0) & (stream_df['GazeStream']['RightPupilDiameter'] > 0), ['time', 'MethodID', 'ModelID', 'LeftPupilDiameter', 'RightPupilDiameter']]\n",
    "    pupil['time'] = pd.to_timedelta(pupil['time'], unit='s')\n",
    "\n",
    "    lum = stream_df['LuminanceStream'].loc[:, ['time', 'MethodID', 'ModelID', 'Luminance']]\n",
    "    lum['time'] = pd.to_timedelta(lum['time'], unit='s')\n",
    "\n",
    "    # Intersection of time stamps\n",
    "    pupil_lum_time_intersection = np.intersect1d(pupil['time'], lum['time'])\n",
    "\n",
    "    # Filter pupil and luminance data by intersection\n",
    "    pupil = pupil[pupil['time'].isin(pupil_lum_time_intersection)].reset_index(drop=True)\n",
    "    lum = lum[lum['time'].isin(pupil_lum_time_intersection)].reset_index(drop=True)\n",
    "\n",
    "    # Combined DataFrame for pupil and luminance\n",
    "    pupil_lum = pd.DataFrame({\n",
    "        'time': pd.to_timedelta(lum['time'], unit='s'),\n",
    "        'luminance': lum['Luminance'],\n",
    "        'pupilDiameter': 0.5 * (pupil['LeftPupilDiameter'] + pupil['RightPupilDiameter']),\n",
    "        'methodID': pupil['MethodID'],\n",
    "        'modelID': pupil['ModelID']\n",
    "    }).resample('0.1s', on='time').mean()\n",
    "\n",
    "    pupil_lum['time'] = pupil_lum.index\n",
    "\n",
    "    return pupil_lum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_calibration_data(pupil_lum_df, stream_df):\n",
    "    calibration_events = stream_df['ExperimentStream'].loc[(stream_df['ExperimentStream']['EventType'] == 'CalibrationColorChange') | (stream_df['ExperimentStream']['SceneEvent'] == 'Calibration') | (stream_df['ExperimentStream']['SceneEvent'] == 'CalibrationComplete'), ['time','SceneEvent', 'EventType']]\n",
    "    calibration_events['time'] = pd.to_timedelta(calibration_events['time'], unit='s')\n",
    "    c_start_times = calibration_events[:8]['time']\n",
    "    c_end_times = calibration_events[1:]['time']\n",
    "    c_start_times.reset_index(drop=True, inplace=True)\n",
    "    c_end_times.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    calib_data = {}\n",
    "    for i in range(8):\n",
    "        calib_data[i] = pupil_lum_df.loc[(pupil_lum_df['time'] >= c_start_times[i]) & (pupil_lum_df['time'] <= c_end_times[i]), ['time','luminance', 'pupilDiameter']]\n",
    "        calib_data[i]['time'] -= calib_data[i]['time'].iloc[0]\n",
    "        calib_data[i] = calib_data[i].loc[(calib_data[i]['time'] >= accom_time), ['luminance', 'pupilDiameter']]\n",
    "\n",
    "    calibration_data = pd.concat(calib_data).groupby(level=0).mean().sort_values(by=['luminance']).reset_index(drop=True)\n",
    "    return calibration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_navigation_data(pupil_lum_df, stream_df, a, b, c):\n",
    "    navigation_events = stream_df['ExperimentStream'].loc[(stream_df['ExperimentStream']['SceneEvent'] == 'NavigationComplete') | (stream_df['ExperimentStream']['SceneEvent'] == 'Navigation_Trial'), ['time','SceneEvent', 'EventType', 'ModelID', 'MethodID']]\n",
    "    navigation_events['time'] = pd.to_timedelta(navigation_events['time'], unit='s')\n",
    "    nav_start_times = navigation_events.loc[navigation_events['SceneEvent'] == 'Navigation_Trial', 'time']\n",
    "    nav_end_times = navigation_events.loc[navigation_events['SceneEvent'] == 'NavigationComplete', 'time']\n",
    "\n",
    "    nav_start_times.reset_index(drop=True, inplace=True)\n",
    "    nav_end_times.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #Correct for occasions when Unity emitted multiple SceneLoaded events for a single trial\n",
    "    if len(nav_start_times) > 8:\n",
    "        nav_diff = nav_start_times.diff().dt.total_seconds()\n",
    "        nav_start_times = nav_start_times.loc[(nav_diff.isnull()) | (nav_diff > 3)]\n",
    "\n",
    "    nav_start_times.reset_index(drop=True, inplace=True)\n",
    "    nav_end_times.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    nav_data = {}\n",
    "    for i in range(8):\n",
    "        nav_data[i] = pupil_lum_df.loc[(pupil_lum_df['time']>=nav_start_times.loc[i]) & (pupil_lum_df['time']<=nav_end_times.loc[i]), ['time', 'methodID', 'modelID', 'luminance', 'pupilDiameter']]\n",
    "        nav_data[i].set_index('time', inplace=True, drop=False)\n",
    "\n",
    "    navigation_data = pd.concat(nav_data, names=['trial'])\n",
    "    navigation_data = navigation_data.groupby(level=0).resample('0.5s', on='time', ).mean()\n",
    "    navigation_data['plr'] = pupil_func(navigation_data['luminance'], a, b, c)\n",
    "    navigation_data['tepr'] = navigation_data['pupilDiameter'] - navigation_data['plr']\n",
    "    \n",
    "    return navigation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_creation_data(pupil_lum_df, stream_df, a, b, c):\n",
    "    creation_events = stream_df['ExperimentStream'].loc[(stream_df['ExperimentStream']['SceneEvent'] == 'Creation_Trial') | (stream_df['ExperimentStream']['SceneEvent'] == 'CreationComplete'), ['time','SceneEvent', 'EventType', 'ModelID', 'MethodID']]\n",
    "    creation_events['time'] = pd.to_timedelta(creation_events['time'], unit='s')\n",
    "    crt_start_times = creation_events.loc[creation_events['SceneEvent'] == 'Creation_Trial', 'time']\n",
    "    crt_end_times = creation_events.loc[creation_events['SceneEvent'] == 'CreationComplete', 'time']\n",
    "\n",
    "    crt_start_times.reset_index(drop=True, inplace=True)\n",
    "    crt_end_times.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #Correct for occasions when Unity emitted multiple SceneLoaded events for a single trial\n",
    "    if len(crt_start_times) > 8:\n",
    "        crt_diff = crt_start_times.diff().dt.total_seconds()\n",
    "        crt_start_times = crt_start_times.loc[(crt_diff.isnull()) | (crt_diff > 3)]\n",
    "\n",
    "    crt_start_times.reset_index(drop=True, inplace=True)\n",
    "    crt_end_times.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    crt_data = {}\n",
    "    for i in range(8):\n",
    "        crt_data[i] = pupil_lum_df.loc[(pupil_lum_df['time']>=crt_start_times.loc[i]) & (pupil_lum_df['time']<=crt_end_times.loc[i]), ['time', 'methodID', 'modelID', 'luminance', 'pupilDiameter']]\n",
    "        crt_data[i].set_index('time', inplace=True, drop=False)\n",
    "\n",
    "    creation_data = pd.concat(crt_data, names=['trial'])\n",
    "    creation_data = creation_data.groupby(level=0).resample('0.5s', on='time', ).mean()\n",
    "    creation_data['plr'] = pupil_func(creation_data['luminance'], a, b, c)\n",
    "    creation_data['tepr'] = creation_data['pupilDiameter'] - creation_data['plr']\n",
    "    \n",
    "    return creation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_discomfort_data(stream_df):\n",
    "    discomfort_values = stream_df['SurveyStream'].loc[stream_df['SurveyStream']['SurveyType'] == 'Discomfort', ['time', 'Value', 'ModelID', 'MethodID']]\n",
    "    discomfort_values['time'] = pd.to_timedelta(discomfort_values['time'], unit='s')\n",
    "    discomfort_values.reset_index(drop=True, inplace=True)\n",
    "    return discomfort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_seq_data(stream_df):\n",
    "    seq_values = stream_df['SurveyStream'].loc[stream_df['SurveyStream']['SurveyType'] == 'SEQ', ['time', 'Value', 'ModelID', 'MethodID']]\n",
    "    seq_values['time'] = pd.to_timedelta(seq_values['time'], unit='s')\n",
    "    seq_values.reset_index(drop=True, inplace=True)\n",
    "    return seq_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = join(getcwd(),'Path_Data')\n",
    "data_files = [join(data_dir, f) for f in listdir(data_dir) if isfile(join(data_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in data_files:\n",
    "    dfs.append(import_data(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = []\n",
    "user_nav_data = []\n",
    "user_crt_data = []\n",
    "user_models_nav = []\n",
    "user_methods_nav = []\n",
    "user_params = []\n",
    "user_calibration = []\n",
    "user_models_crt = []\n",
    "user_methods_crt = []\n",
    "user_seq = []\n",
    "user_discomfort = []\n",
    "\n",
    "for df in dfs:\n",
    "    user_ids.append(df['ExperimentStream']['UserID'][0]) \n",
    "\n",
    "    pupil_lum_df = process_gaze_luminance_data(df)\n",
    "    calibration_data = process_calibration_data(pupil_lum_df, df)\n",
    "\n",
    "    # Fit pupil response to luminance\n",
    "    x_data = calibration_data['luminance']\n",
    "    y_data = calibration_data['pupilDiameter']\n",
    "    exp_mod = Model(pupil_func)\n",
    "    params = exp_mod.make_params(a=1, b=4, c=0)\n",
    "    result = exp_mod.fit(y_data, params, x=x_data)\n",
    "    a = result.params['a'].value\n",
    "    b = result.params['b'].value\n",
    "    c = result.params['c'].value\n",
    "\n",
    "    user_params.append(pd.DataFrame({'params': [a, b, c]}, index=['a', 'b', 'c']))\n",
    "\n",
    "    navigation_data = process_navigation_data(pupil_lum_df, df, a, b, c)\n",
    "    navigation_avg = navigation_data.groupby(level=0).mean()\n",
    "    navigation_avg.drop(columns=['luminance'], inplace=True)\n",
    "\n",
    "    user_nav_data.append(navigation_avg)\n",
    "\n",
    "    models = navigation_avg.reset_index(drop=True)\n",
    "    model_avg = models.groupby(['modelID']).mean()\n",
    "    model_avg.drop(columns=['methodID'], inplace=True)\n",
    "    methods = navigation_avg.reset_index(drop=True)\n",
    "    method_avg = methods.groupby(['methodID']).mean()\n",
    "    method_avg.drop(columns=['modelID'], inplace=True)\n",
    "\n",
    "    user_models_nav.append(model_avg)\n",
    "    user_methods_nav.append(method_avg)\n",
    "\n",
    "    creation_data = process_creation_data(pupil_lum_df, df, a, b, c)\n",
    "    creation_avg = creation_data.groupby(level=0).mean()\n",
    "    creation_avg.drop(columns=['luminance'], inplace=True)\n",
    "\n",
    "    user_crt_data.append(creation_avg)\n",
    "\n",
    "    models = creation_avg.reset_index(drop=True)\n",
    "    model_avg = models.groupby(['modelID']).mean()\n",
    "    model_avg.drop(columns=['methodID'], inplace=True)\n",
    "    methods = creation_avg.reset_index(drop=True)\n",
    "    method_avg = methods.groupby(['methodID']).mean()\n",
    "    method_avg.drop(columns=['modelID'], inplace=True)\n",
    "\n",
    "    user_models_crt.append(model_avg)\n",
    "    user_methods_crt.append(method_avg)\n",
    " \n",
    "    user_discomfort.append(process_discomfort_data(df))\n",
    "    user_seq.append(process_seq_data(df))\n",
    "\n",
    "params = pd.concat(user_params, keys=user_ids, names=['UserID'])\n",
    "model_data_nav = pd.concat(user_models_nav, keys=user_ids, names=['UserID'])\n",
    "method_data_nav = pd.concat(user_methods_nav, keys=user_ids, names=['UserID'])\n",
    "model_data_crt = pd.concat(user_models_crt, keys=user_ids, names=['UserID'])\n",
    "method_data_crt = pd.concat(user_methods_crt, keys=user_ids, names=['UserID'])\n",
    "nav_data = pd.concat(user_nav_data, keys=user_ids, names=['UserID'])\n",
    "crt_data = pd.concat(user_crt_data, keys=user_ids, names=['UserID'])\n",
    "seq_data = pd.concat(user_seq, keys=user_ids, names=['UserID'])\n",
    "discomfort_data = pd.concat(user_discomfort, keys=user_ids, names=['UserID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation Workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 2\n",
      "Mean = 0.491\n",
      "Median = 0.464\n",
      "Std = 0.275\n",
      "Method 3\n",
      "Mean = 0.490\n",
      "Median = 0.431\n",
      "Std = 0.286\n",
      "Shapiro-Wilk T = 0.928, p = 0.034\n",
      "Shapiro-Wilk M2 = 0.954, p = 0.548\n",
      "Shapiro-Wilk M3 = 0.887, p = 0.051\n",
      "Wilcoxon = 67.000, p = 0.980\n",
      "Paired t-test = 0.021, p = 0.984\n"
     ]
    }
   ],
   "source": [
    "model_nav_tepr = model_data_nav.loc[(slice(None), slice(None)), 'tepr']\n",
    "method_nav_tepr = method_data_nav.loc[(slice(None), slice(None)), 'tepr']\n",
    "\n",
    "method_tepr_2 = method_data_nav.loc[(slice(None), 2), 'tepr']\n",
    "method_tepr_3 = method_data_nav.loc[(slice(None), 3), 'tepr']\n",
    "method_tepr_2.reset_index(drop=True, inplace=True)\n",
    "method_tepr_3.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#shapiro-wilk test\n",
    "statt, pt = stats.shapiro(method_nav_tepr)\n",
    "stat0, p0 = stats.shapiro(method_tepr_2)\n",
    "stat1, p1 = stats.shapiro(method_tepr_3)\n",
    "\n",
    "#wilcoxon test\n",
    "stat, p = stats.wilcoxon(method_tepr_2, method_tepr_3)\n",
    "\n",
    "#paired t-test\n",
    "t_stat, p_val = stats.ttest_rel(method_tepr_2, method_tepr_3)\n",
    "\n",
    "#descriptive stats (avg, median, std)\n",
    "print('Method 2')\n",
    "print('Mean = %.3f' % method_tepr_2.mean())\n",
    "print('Median = %.3f' % method_tepr_2.median())\n",
    "print('Std = %.3f' % method_tepr_2.std())\n",
    "\n",
    "print('Method 3')\n",
    "print('Mean = %.3f' % method_tepr_3.mean())\n",
    "print('Median = %.3f' % method_tepr_3.median())\n",
    "print('Std = %.3f' % method_tepr_3.std())\n",
    "\n",
    "print('Shapiro-Wilk T = %.3f, p = %.3f' % (statt, pt))\n",
    "print('Shapiro-Wilk M2 = %.3f, p = %.3f' % (stat0, p0))\n",
    "print('Shapiro-Wilk M3 = %.3f, p = %.3f' % (stat1, p1))\n",
    "print('Wilcoxon = %.3f, p = %.3f' % (stat, p))\n",
    "print('Paired t-test = %.3f, p = %.3f' % (t_stat, p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation Workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 0\n",
      "Mean = 0.557\n",
      "Median = 0.581\n",
      "Std = 0.300\n",
      "Method 1\n",
      "Mean = 0.430\n",
      "Median = 0.473\n",
      "Std = 0.275\n",
      "Shapiro-Wilk T = 0.963, p = 0.33435\n",
      "Shapiro-Wilk M2 = 0.962, p = 0.69258\n",
      "Shapiro-Wilk M3 = 0.951, p = 0.50215\n",
      "Wilcoxon = 5.000, p = 0.00031\n",
      "Paired t-test = 3.327, p = 0.00460\n"
     ]
    }
   ],
   "source": [
    "model_crt_tepr = model_data_crt.loc[(slice(None), slice(None)), 'tepr']\n",
    "method_crt_tepr = method_data_crt.loc[(slice(None), slice(None)), 'tepr']\n",
    "\n",
    "method_tepr_0 = method_data_crt.loc[(slice(None), 0), 'tepr']\n",
    "method_tepr_1 = method_data_crt.loc[(slice(None), 1), 'tepr']\n",
    "method_tepr_0.reset_index(drop=True, inplace=True)\n",
    "method_tepr_1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#shapiro-wilk test\n",
    "statt, pt = stats.shapiro(method_crt_tepr)\n",
    "stat0, p0 = stats.shapiro(method_tepr_0)\n",
    "stat1, p1 = stats.shapiro(method_tepr_1)\n",
    "\n",
    "#wilcoxon test\n",
    "stat, p = stats.wilcoxon(method_tepr_0, method_tepr_1)\n",
    "\n",
    "#paired t-test\n",
    "t_stat, p_val = stats.ttest_rel(method_tepr_0, method_tepr_1)\n",
    "\n",
    "#descriptive stats (avg, median, std)\n",
    "print('Method 0')\n",
    "print('Mean = %.3f' % method_tepr_0.mean())\n",
    "print('Median = %.3f' % method_tepr_0.median())\n",
    "print('Std = %.3f' % method_tepr_0.std())\n",
    "\n",
    "print('Method 1')\n",
    "print('Mean = %.3f' % method_tepr_1.mean())\n",
    "print('Median = %.3f' % method_tepr_1.median())\n",
    "print('Std = %.3f' % method_tepr_1.std())\n",
    "\n",
    "print('Shapiro-Wilk T = %.3f, p = %.5f' % (statt, pt))\n",
    "print('Shapiro-Wilk M2 = %.3f, p = %.5f' % (stat0, p0))\n",
    "print('Shapiro-Wilk M3 = %.3f, p = %.5f' % (stat1, p1))\n",
    "print('Wilcoxon = %.3f, p = %.5f' % (stat, p))\n",
    "print('Paired t-test = %.3f, p = %.5f' % (t_stat, p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discomfort Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 2\n",
      "Mean = 1.850\n",
      "Median = 0.900\n",
      "Std = 1.994\n",
      "Method 3\n",
      "Mean = 2.325\n",
      "Median = 2.000\n",
      "Std = 2.374\n",
      "Shapiro-Wilk T = 0.963, p = 0.33435\n",
      "Shapiro-Wilk M2 = 0.820, p = 0.00503\n",
      "Shapiro-Wilk M3 = 0.874, p = 0.03184\n",
      "Paired t-test = -2.328, p = 0.034\n"
     ]
    }
   ],
   "source": [
    "discomfort_method = discomfort_data.loc[(slice(None), slice(None)), ['Value', 'MethodID', 'ModelID']]\n",
    "discomfort_method['Value'] = discomfort_method['Value'].astype(float)\n",
    "discomfort_method['MethodID'] = discomfort_method['MethodID'].astype(float)\n",
    "discomfort_method['ModelID'] = discomfort_method['ModelID'].astype(float)\n",
    "\n",
    "# find the average for each user for each method\n",
    "discomfort_avg = discomfort_method.groupby(['UserID', 'MethodID']).mean()\n",
    "discomfort_method_2 = discomfort_avg.loc[(slice(None), 2), 'Value']\n",
    "discomfort_method_3 = discomfort_avg.loc[(slice(None), 3), 'Value']\n",
    "\n",
    "#shapiro-wilk test\n",
    "stat2, p2 = stats.shapiro(discomfort_method_2)\n",
    "stat3, p3 = stats.shapiro(discomfort_method_3)\n",
    "\n",
    "#wilcoxon test\n",
    "#stat, p = stats.wilcoxon(discomfort_method_2, discomfort_method_3, zero_method='zsplit')\n",
    "\n",
    "#paired t-test\n",
    "t_stat, p_val = stats.ttest_rel(discomfort_method_2, discomfort_method_3)\n",
    "\n",
    "#descriptive stats (avg, median, std)\n",
    "print('Method 2')\n",
    "print('Mean = %.3f' % discomfort_method_2.mean())\n",
    "print('Median = %.3f' % discomfort_method_2.median())\n",
    "print('Std = %.3f' % discomfort_method_2.std())\n",
    "\n",
    "print('Method 3')\n",
    "print('Mean = %.3f' % discomfort_method_3.mean())\n",
    "print('Median = %.3f' % discomfort_method_3.median())\n",
    "print('Std = %.3f' % discomfort_method_3.std())\n",
    "\n",
    "print('Shapiro-Wilk T = %.3f, p = %.5f' % (statt, pt))\n",
    "print('Shapiro-Wilk M2 = %.3f, p = %.5f' % (stat2, p2))\n",
    "print('Shapiro-Wilk M3 = %.3f, p = %.5f' % (stat3, p3))\n",
    "#print('Wilcoxon = %.3f, p = %.3f' % (stat, p))\n",
    "print('Paired t-test = %.3f, p = %.3f' % (t_stat, p_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 0\n",
      "Mean = 0.547\n",
      "Median = 0.125\n",
      "Std = 0.660\n",
      "Method 1\n",
      "Mean = 0.938\n",
      "Median = 0.500\n",
      "Std = 1.124\n",
      "Method 2\n",
      "Mean = 0.422\n",
      "Median = 0.125\n",
      "Std = 0.604\n",
      "Method 3\n",
      "Mean = 0.797\n",
      "Median = 0.500\n",
      "Std = 0.877\n",
      "Shapiro-Wilk = 0.846, p = 0.00000\n",
      "Wilcoxon 01 = 22.500, p = 0.08693\n",
      "Wilcoxon 23 = 41.000, p = 0.24756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\court\\Documents\\Github Projects\\CurrentStudy\\.venv\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n"
     ]
    }
   ],
   "source": [
    "seq_method = seq_data.loc[(slice(None), slice(None)), ['Value', 'MethodID', 'ModelID']]\n",
    "seq_method['Value'] = seq_method['Value'].astype(float)\n",
    "seq_method['MethodID'] = seq_method['MethodID'].astype(float)\n",
    "seq_method['ModelID'] = seq_method['ModelID'].astype(float)\n",
    "\n",
    "# find the average for each user for each method\n",
    "seq_avg = seq_method.groupby(['UserID', 'MethodID']).mean()\n",
    "seq_method_0 = seq_avg.loc[(slice(None), 0), 'Value']\n",
    "seq_method_1 = seq_avg.loc[(slice(None), 1), 'Value']\n",
    "seq_method_2 = seq_avg.loc[(slice(None), 2), 'Value']\n",
    "seq_method_3 = seq_avg.loc[(slice(None), 3), 'Value']\n",
    "\n",
    "#shapiro-wilk test\n",
    "statt, pt = stats.shapiro(seq_method)\n",
    "\n",
    "#wilcoxon test\n",
    "statw_01, p_w_01 = stats.wilcoxon(seq_method_0, seq_method_1, zero_method='pratt')\n",
    "statw_23, p_w_23 = stats.wilcoxon(seq_method_2, seq_method_3, zero_method='pratt')\n",
    "\n",
    "#paired t-test\n",
    "t_stat, p_val = stats.ttest_rel(seq_method_0, seq_method_1)\n",
    "t_stat2, p_val2 = stats.ttest_rel(seq_method_2, seq_method_3)\n",
    "\n",
    "#descriptive stats (avg, median, std)\n",
    "print('Method 0')\n",
    "print('Mean = %.3f' % seq_method_0.mean())\n",
    "print('Median = %.3f' % seq_method_0.median())\n",
    "print('Std = %.3f' % seq_method_0.std())\n",
    "\n",
    "print('Method 1')\n",
    "print('Mean = %.3f' % seq_method_1.mean())\n",
    "print('Median = %.3f' % seq_method_1.median())\n",
    "print('Std = %.3f' % seq_method_1.std())\n",
    "\n",
    "print('Method 2')\n",
    "print('Mean = %.3f' % seq_method_2.mean())\n",
    "print('Median = %.3f' % seq_method_2.median())\n",
    "print('Std = %.3f' % seq_method_2.std())\n",
    "\n",
    "print('Method 3')\n",
    "print('Mean = %.3f' % seq_method_3.mean())\n",
    "print('Median = %.3f' % seq_method_3.median())\n",
    "print('Std = %.3f' % seq_method_3.std())\n",
    "\n",
    "print('Shapiro-Wilk = %.3f, p = %.5f' % (statt, pt))\n",
    "\n",
    "print('Wilcoxon 01 = %.3f, p = %.5f' % (statw_01, p_w_01))\n",
    "print('Wilcoxon 23 = %.3f, p = %.5f' % (statw_23, p_w_23))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
